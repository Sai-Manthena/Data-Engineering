# Database purpose and Analytical goals.

Created a postgres database by defining Fact and dimension Tables for analysis on songs and user activity on sparkify music streaming app. This will helps us to particularly analze songs users on listening.
 

# Database schema design and ETL Pipeline.

Source data: 
  1. Song Dataset: a subset of million song dataset containing.Each file is in JSON format and contains metadata about a song and the artist of that song
  2. log Dataset: log files in JSON format generated by event simulator based on Song dataset.

Schema Design: The Goal of the design is to organize the data to avoid duplication of fields and repeating data, and to ensure quality of the data.
Followed "Normalization" process to distribute the data into individual entities(Songs,Users,Artists,Time).This way it is easier to track changes to any data without effecting whole database. 
  for ex: if we need to update Artist location. we can just update artist table.
  
  Fact table contains all the metrics data and using primary_key we can make joins to dimension tables to get necessary info. 
  
  
  Fact Table
   1. songplays: records in log data associated with song plays i.e. records with page NextSong
   - songplay_id(Primary Key), start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  Dimension Tables
   1. users:users in the app
   - user_id(Primary_Key), first_name, last_name, gender, level
   2. songs:songs in music database
   - song_id(Primary_Key), title, artist_id, year, duration
   3. artists:artists in music database
   - artist_id(Primary_Key), name, location, latitude, longitude
   4. time:timestamps of records in songplays broken down into specific units
   - start_time, hour, day, week, month, year, weekday
   
  For EX: To Identify how long each user is spending on the website. 
  
  Select 
  user_di,
  first_name,
  last_name,
  sum(session_id)
  from 
  songplays a inner join 
  users b on a.user_id = b.user_id 
  group by 
   user_di, first_name,last_name ;
   

   
 ETL PipeLine:
 - We created ETL pipeline through Python and Sql queries. 
 - Most of the data files are in JSON format.So in order to read the data we need to create Pandas dataframe.Inorder to do this we first import JSON and Pandas libraries.
 - Create_tables.py scripts creates all the tables discussed above in Schema design phase. 
 - etl.py scripts makes connection to sparkify database 1st and process the file from song_data and log_data directories to load into fact and Dimesion tables.

