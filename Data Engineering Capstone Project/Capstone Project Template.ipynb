{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "import  pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "#from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Project: Capstone Project.\n",
    "# Scope of the Project:\n",
    "Creating a ETL Pipeline using Spark.To achieve this we use Immigration data which comes from US National Tourism and Trade Office , City Demophrapics data which comes from Opensoft.\n",
    "\n",
    "Immigration data has detailed information of people arriving to US through Air, Sea, Land. for ex: Arrival City, Visa type, arrival date and time, departure date and time etc, \n",
    "City Demographic data file has information related to each city which includes Population,State code, Median Population , native born, foriegn born etc. \n",
    "\n",
    "Description:\n",
    "To complete the project, we will need to load data from Harddisk, process the data into analytics tables using Spark.\n",
    "If the data is biggerthan disk we will deploy spark process on a cluster using AWS and load the data on to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "Immig_data =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "city_data=spark.read.csv('us-cities-demographics.csv',sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Immig_data.write.mode('overwrite').parquet(\"sas_data\")\n",
    "Immmig_data=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immmig_data.printSchema()\n",
    "Immmig_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_data.printSchema()\n",
    "city_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Exploring the data and identifying the data issues.\n",
    "Immmig_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Based on the Cicid count which matches with Total cound of Immigration data we can consider cicid to unique with no duplictae values\n",
    "ccid_count = State_dtls.select(countDistinct(\"cicid\"))\n",
    "ccid_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#checking below if we have any missing values for the required columns in Immigration data. \n",
    "Immmig_data.describe(\"i94addr\",\"i94yr\",\"i94mon\",\"arrdate\",\"visatype\",\"i94mode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping rows with null values\n",
    "Immmig_data = Immmig_data.dropna(how = \"any\", subset = [\"cicid\", \"i94addr\",\"i94mode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropped all the rows with no address as our analysis is only based on the state codes in US. \n",
    "Immmig_data.describe(\"cicid\",\"i94addr\",\"i94yr\",\"i94mon\",\"arrdate\",\"visatype\",\"i94mode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Checking if there are any Empty Values in Cicid column\n",
    "Immmig_data.select(\"cicid\").dropDuplicates().sort(\"cicid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Changing the data types from Double to Int for all the Numeric Fields.I could have done this by defining a new schema for \\\n",
    "#Immigration data and infering this schema when importing the data.\n",
    "#Converting the SAS Numeric date form to actual date form.\n",
    "Immmig_data = Immmig_data.withColumn('i94yr', expr(\"cast(i94yr  as int)\")) \\\n",
    ".withColumn('i94mon', expr(\"cast(i94mon  as int)\")) \\\n",
    ".withColumn('arrdate', expr(\"date_add('1960-01-01',arrdate)\")) \\\n",
    ".withColumn('depdate', expr(\"date_add('1960-01-01',depdate)\")) \\\n",
    ".withColumn('cicid', expr(\"cast(cicid  as int)\")) \\\n",
    ".withColumn('i94mode', expr(\"cast(i94mode  as int)\")) \\\n",
    ".withColumn('i94bir', expr(\"cast(i94bir  as int)\")) \\\n",
    ".withColumn('i94visa', expr(\"cast(i94visa  as int)\")) \\\n",
    ".withColumn('biryear', expr(\"cast(biryear  as int)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#checking to see how data is distributed as per the i94addr\n",
    "Immmig_data.groupby(\"i94addr\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Exploring City Data\n",
    "city_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_data.describe(\"City\",\"State\",\"Male Population\",\"Female Population\",\"State Code\",\"Median Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping Null rows if there are any.\n",
    "city_data = city_data.dropna(how = \"any\", subset = [\"City\", \"State\",\"State Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#validating why we have duplicate values for each city and State\n",
    "city_data.select(\"City\", \"State\",\"State Code\",\"Total Population\",\"Race\",\"Count\").filter(city_data[\"City\"].isin([\"Lawrence\"])).sort(\"Count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#changing the name of the columns to remove trailing whitespaces.\n",
    "city_data = city_data.selectExpr(\"City as city\",\"State as state\",\"`Median Age` as Median_age\",\\\n",
    "                                 \"`Male Population` as Male_Population\",\"`Female Population` as Female_Population\",\\\n",
    "                                 \"`Total Population` as Total_Population\",\"`Number of Veterans` as Total_Veterans\",\\\n",
    "                                 \"`Foreign-born` as Foreign_born\",\"`Average Household Size` as Average_Household_Size\",\\\n",
    "                                 \"`State Code` as State_Code\",\"Race as race\",\"count as count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Schema for Imigration Visit Analysis\n",
    "Using the Immigration and City Demographic datasets,I have created a star schema optimized for queries on Immigration Visit analysis. This includes the following tables.\n",
    "\n",
    "The Goal of the design is to organize the data to avoid duplication of fields and repeating data, and to ensure quality of the data.\n",
    "Followed \"Normalization\" process to distribute the data into individual entities.\n",
    "\n",
    "Fact Table\n",
    "Immig Visit - records in Immigartion data associated with Immigration visits \n",
    "  Immig_Event_id: long \n",
    "  visitor_id: integer \n",
    "  Date: date \n",
    "  month: integer \n",
    "  year: integer \n",
    "  i94mode: integer \n",
    "  i94visa: integer \n",
    "  Match_flag: string \n",
    "  i94addr: string \n",
    "  \n",
    "Dimension Tables\n",
    "Immig Details - Vistors entering the country\n",
    "  visitor_id: integer (nullable = true)\n",
    "  i94yr: integer \n",
    "  i94mon: integer\n",
    "  arrival_date: date \n",
    "  i94mode: string \n",
    "  departure_date: date \n",
    "  Age: integer \n",
    "  i94visa: string \n",
    "  create_dt: string \n",
    "  visapost: string \n",
    "  birthyr: integer \n",
    "  gender: string \n",
    "  airline: string \n",
    "  visatype: string\n",
    " \n",
    "State Details - State Details\n",
    "  state_id: long \n",
    "  state: string \n",
    "  State_Code: string \n",
    "  city: string \n",
    "  \n",
    "Date Table - Date of records in Immigration visits broken down into specific units\n",
    "  Date: date \n",
    "  day: integer \n",
    "  week: integer \n",
    "  month: integer \n",
    "  year: integer \n",
    "  weekday: integer\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    " * Processing both immigration and City demographics data to create analytics tables\n",
    " * Creat Immigration view to store data in staging table\n",
    " * Create City Data view to store data in staging table\n",
    " * extract columns from Immigration view to create Immig_dtls table\n",
    " * extract columns from State Data view to create State details table\n",
    " * extract columns from Immigration view to create Date table\n",
    " * extract columns from Immigration view to create Date table\n",
    " * extract columns from Immigration and state to create Immigration event Fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Processing both immigration and City data to create analytics tables\n",
    "#Creating Immigration view to store data in staging table\n",
    "#Creating City Data view to store data in staging table\n",
    "Immmig_data.createOrReplaceTempView(\"stg_Immig_data\")\n",
    "city_data.createOrReplaceTempView(\"stg_city_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create Immig_dtls table\n",
    "Immig_dtls = spark.sql(\"\"\"\n",
    "             select Distinct \n",
    "             a.cicid as visitor_id,\n",
    "             a.i94yr,\n",
    "             a.i94mon,\n",
    "             a.arrdate as arrival_date,\n",
    "             Case when a.i94mode = 1 then 'AIR' \\\n",
    "             when a.i94mode = 2 then 'Sea'\\\n",
    "             when a.i94mode = 3 then 'Land' else 'other' end as i94mode,\n",
    "             a.depdate as departure_date,\n",
    "             a.i94bir as Age,\n",
    "             Case when a.i94visa = 1 then 'Business'\\\n",
    "                  when a.i94visa = 2 then 'Pleasure' \\\n",
    "                  when a.i94visa = 3 then 'Student' else 'other' end as i94visa,\n",
    "             a.dtadfile as create_dt,\n",
    "             a.visapost ,\n",
    "             a.biryear as birthyr,\n",
    "             a.gender,\n",
    "             a.airline,\n",
    "             a.visatype\n",
    "             from\n",
    "             stg_Immig_data  a left join\n",
    "             stg_city_data b on a.i94addr = b.State_Code\n",
    "             where \n",
    "             a.cicid is not null \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immig_dtls.createOrReplaceTempView(\"dim_Immig_dtls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create State_dtls table\n",
    "State_dtls = spark.sql(\"\"\"\n",
    "             select Distinct\n",
    "             monotonically_increasing_id() as state_id,\n",
    "             a.state as state,\n",
    "             a.State_Code,\n",
    "             a.city\n",
    "             from\n",
    "             stg_city_data  a \n",
    "             where\n",
    "             a.State_Code is not null\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "State_dtls.createOrReplaceTempView(\"dim_State_dtls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create Date_table \n",
    "Date_table = spark.sql(\"\"\"\n",
    "             select Distinct\n",
    "             a.arrdate as Date,\n",
    "             hour(a.arrdate) as hour,\n",
    "             dayofmonth(a.arrdate) as day,\n",
    "             weekofyear(a.arrdate) as week,\n",
    "             month(a.arrdate) as month,\n",
    "             year(a.arrdate) as year,\n",
    "             dayofweek(a.arrdate) as weekday\n",
    "             from\n",
    "             stg_Immig_data  a \n",
    "             where\n",
    "             a.arrdate is not null\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Date_table.createOrReplaceTempView(\"dim_Date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immig_visit = spark.sql(\"\"\"\n",
    "             select Distinct \n",
    "             monotonically_increasing_id() as Immig_visit_id,\n",
    "             a.cicid as visitor_id,\n",
    "             a.arrdate as Date,\n",
    "             month(a.arrdate) as month,\n",
    "             year(a.arrdate) as year,\n",
    "             a.i94mode,\n",
    "             a.i94visa,\n",
    "             a.matflag as Match_flag,\n",
    "             Case when a.i94addr = b.State_Code then a.i94addr \\\n",
    "             else '99' end as i94State\n",
    "             from\n",
    "             stg_Immig_data  a left join\n",
    "             stg_city_data b on a.i94addr = b.State_Code\n",
    "             where \n",
    "             a.cicid is not null \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immig_visit.createOrReplaceTempView(\"Fact_Immig_visit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Immigration Source/Analytics table count check to ensure completness\n",
    "Immmig_data.count()\n",
    "Immig_dtls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#City Demographic Source/Analytics table count check to ensure completness\n",
    "city_data.count()\n",
    "State_dtls.select(countDistinct(\"state_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#making sure analytic tables are created as per Integrity Constraints by running below commands\n",
    "Immig_visit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Date_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Running unit tests to make sure that data loaded properly. \n",
    "Immig_dtls.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immig_visit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Date_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "State_dtls.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Run below query to make sure all the joins work\n",
    "spark.sql(\"\"\"\n",
    "             select\n",
    "             count(Distinct a.visitor_id),\n",
    "             b.i94mode,\n",
    "             c.week,\n",
    "             d.state\n",
    "             from\n",
    "             Fact_Immig_visit  a inner join \n",
    "             dim_Immig_dtls b on a.visitor_id = b.visitor_id inner join\n",
    "             dim_Date_table c on a.date = c.date inner join \n",
    "             dim_State_dtls d on d.state_code = i94State\n",
    "             where\n",
    "             a.i94mode = 1\n",
    "             group by \n",
    "             a.i94mode,c.week,d.state\n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I have created separate Data Dictionary file saved as Data_Dictionary_Capstone_Project.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Rationale for the choice of tools and Technologies for the project\n",
    " * As the immigration data is huge i have used Spark to process the data.\n",
    " * Spark tries to process the data In memory whenever possible before writing the results to HardDisk.This helps us to process the data Faster.\n",
    " * As i have processed Immigration data only for the month of April , i avoided using  s3(Data Lake) to store the data. \n",
    " * If i have opted to process all the datafiles for the Year 2016, 1ST i would have stored the data in S3.\n",
    " * Processed them through Spark and stored back my analytics data to the S3. \n",
    " * Using S3 also involves a bit of cost, so i tried to avoid the cost. \n",
    " * Used PySpark sql to run the functions \n",
    " * I could have used Python and Pandas Libraries to process the data as well, as i have only dealt with April month data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Propose how often the data should be updated and why.\n",
    " * Data needs to be processed daily as batch job.Visitors arrive through immigration everyday.\n",
    " * As per the File , records are also added daily to the Immigartion file.\n",
    " * This helps us to have uptodate data and find daily patterns if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The data was increased by 100x\n",
    "there are couple of ways to handle this\n",
    " 1. Move the data to cloud source (S3,Azure,GPS) as the traditional storage systems are getting Expensive.\n",
    " 2. Increase the Nodes to distribute the data for better performance.\n",
    " 3. Enhancing the data pipelines  by partitioning the data to avoid data skewdness.\n",
    " 4. Filtering the data to include only required observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    "I would use Apache Airflow to schedule a data Pipeline that would update the data daily by 6:30 am. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The database needed to be accessed by 100+ people\n",
    "1. Move the data to cloud source (S3,Azure,GPS) as they automatically scales to high request rates \n",
    " for ex: In terms of S3 we can read or write performances by parallelizing reads.\n",
    "        Also Combine Amazon S3 (Storage) and Amazon EC2 (Compute) in the Same AWS Region\n",
    "        (https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
